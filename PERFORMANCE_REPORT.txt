Performance Report: From Single Models to Ensemble

Overview

This report analyzes the performance evolution of Hemoglobin (Hb) regression prediction models using Nail and Conjunctiva images, from single modality through multimodal fusion to ensemble, step by step.

================================================================================

1. Single Model (Single Modality)

1.1 Nail Single Model

Best Performing Model:
- Backbone: timm/rexnetr_300.sw_in12k
- Test MAE: 0.792 g/dL
- Test R²: 0.437
- Acc@0.5: 0.446
- Acc@1.0: 0.663
- Acc@2.0: 0.940

Top 5 Model Performance:

Backbone                                    | MAE  | R²   | Acc@0.5 | Acc@1.0 | Acc@2.0
--------------------------------------------|------|------|---------|---------|--------
rexnetr_300.sw_in12k                        | 0.792| 0.437| 0.446   | 0.663   | 0.940
convnext_base.clip_laion2b_augreg_ft_in12k  | 0.826| 0.438| 0.301   | 0.687   | 0.952
resnetaa101d.sw_in12k                       | 0.866| 0.374| 0.373   | 0.614   | 0.952
convnext_small.in12k                        | 0.872| 0.440| 0.349   | 0.614   | 0.940
efficientnet_b5.sw_in12k                    | 0.878| 0.442| 0.325   | 0.627   | 0.928

1.2 Conjunctiva Single Model

Best Performing Model:
- Backbone: timm/efficientnet_b5.sw_in12k
- Test MAE: 0.935 g/dL
- Test R²: 0.356
- Acc@0.5: 0.421
- Acc@1.0: 0.716
- Acc@2.0: 0.884

Top 5 Model Performance:

Backbone                      | MAE  | R²   | Acc@0.5 | Acc@1.0 | Acc@2.0
------------------------------|------|------|---------|---------|--------
efficientnet_b5.sw_in12k      | 0.935| 0.356| 0.421   | 0.716   | 0.884
rexnetr_300.sw_in12k          | 0.950| 0.241| 0.340   | 0.660   | 0.910
convnext_small.in12k          | 0.964| 0.387| 0.347   | 0.663   | 0.884
coatnet_2_rw_224.sw_in12k     | 1.089| 0.360| 0.360   | 0.584   | 0.820
coatnet_rmlp_1_rw2_224.sw_in12k| 1.140| 0.365| 0.270   | 0.539   | 0.888

1.3 Single Model Summary

- Nail Best Performance: MAE 0.792, R² 0.437
- Conjunctiva Best Performance: MAE 0.935, R² 0.356
- Average Performance: MAE ~0.86, R² ~0.40

================================================================================

2. Phase 1 Fusion (Basic Fusion)

Phase 1 used a basic fusion method that combines the last layer features of Nail and Conjunctiva.

2.1 w_demo (With Demographics)

Best Performing Model:
- Fold: 2
- Nail Backbone: convnext_small.in12k
- Conj Backbone: efficientnet_b5.sw_in12k
- Test MAE: 0.208 g/dL
- Test R²: 0.020
- Acc@0.5: 1.000
- Acc@1.0: 1.000
- Acc@2.0: 1.000

Top 5 Model Performance:

Fold | Nail Model              | Conj Model              | MAE  | R²    | Acc@0.5 | Acc@1.0 | Acc@2.0
-----|-------------------------|-------------------------|------|-------|---------|---------|--------
2    | convnext_small.in12k    | efficientnet_b5.sw_in12k| 0.208| 0.020 | 1.000   | 1.000   | 1.000
4    | convnext_small.in12k    | convnext_small.in12k    | 0.655| 0.512 | 0.625   | 0.625   | 1.000
3    | convnext_small.in12k    | efficientnet_b5.sw_in12k| 0.704| -0.303| 0.625   | 0.750   | 0.813
4    | convnext_small.in12k    | efficientnet_b5.sw_in12k| 0.767| 0.342 | 0.563   | 0.625   | 1.000
3    | convnext_small.in12k    | convnext_small.in12k    | 0.797| -0.595| 0.500   | 0.750   | 0.750

2.2 wo_demo (Without Demographics)

Best Performing Model:
- Fold: 2
- Nail Backbone: convnext_small.in12k
- Conj Backbone: efficientnet_b5.sw_in12k
- Test MAE: 0.217 g/dL
- Test R²: -0.049
- Acc@0.5: 0.938
- Acc@1.0: 1.000
- Acc@2.0: 1.000

Top 5 Model Performance:

Fold | Nail Model              | Conj Model              | MAE  | R²    | Acc@0.5 | Acc@1.0 | Acc@2.0
-----|-------------------------|-------------------------|------|-------|---------|---------|--------
2    | convnext_small.in12k    | efficientnet_b5.sw_in12k| 0.217| -0.049| 0.938   | 1.000   | 1.000
4    | convnext_small.in12k    | convnext_small.in12k    | 0.657| 0.520 | 0.625   | 0.688   | 1.000
3    | convnext_small.in12k    | efficientnet_b5.sw_in12k| 0.702| -0.316| 0.688   | 0.750   | 0.750
3    | convnext_small.in12k    | convnext_small.in12k    | 0.763| -0.170| 0.625   | 0.688   | 1.000
4    | convnext_small.in12k    | efficientnet_b5.sw_in12k| 0.779| 0.343 | 0.500   | 0.625   | 1.000

2.3 Phase 1 Summary

- w_demo Best Performance: MAE 0.208, R² 0.020 (Fold 2)
- wo_demo Best Performance: MAE 0.217, R² -0.049 (Fold 2)
- Improvement vs Single Model: MAE reduced by approximately 75% (0.792 → 0.208)
- Demographics Effect: w_demo slightly outperforms wo_demo (MAE 0.208 vs 0.217)

================================================================================

3. Phase 2 Fusion (Multi-level Feature Fusion)

Phase 2 used Phase 1 models as pretrained backbones and applied multi-level feature fusion that utilizes both intermediate and final layer features.

3.1 w_demo (With Demographics)

Selected Models:

1. Minimum MAE Model:
   - Fold: 3
   - Nail Backbone: timm_convnext_small.in12k
   - Conj Backbone: timm_efficientnet_b5.sw_in12k
   - Val MAE: 0.655 g/dL
   - Val R²: 0.059
   - Acc@0.5: 0.563
   - Acc@1.0: 0.750
   - Acc@2.0: 0.875

2. Maximum R² Model:
   - Fold: 4
   - Nail Backbone: timm_convnext_small.in12k
   - Conj Backbone: timm_convnext_small.in12k
   - Val MAE: 0.722 g/dL
   - Val R²: 0.478
   - Acc@0.5: 0.500
   - Acc@1.0: 0.625
   - Acc@2.0: 1.000

3.2 wo_demo (Without Demographics)

Selected Models:

1. Minimum MAE Model (Same as Maximum R²):
   - Fold: 0
   - Nail Backbone: timm_convnext_small.in12k
   - Conj Backbone: timm_convnext_small.in12k
   - Val MAE: 0.813 g/dL
   - Val R²: 0.411
   - Acc@0.5: 0.350
   - Acc@1.0: 0.750
   - Acc@2.0: 0.950

3.3 Phase 2 Summary

- w_demo Best Performance: Val MAE 0.655, Val R² 0.478
- wo_demo Best Performance: Val MAE 0.813, Val R² 0.411
- Compared to Phase 1: More stable performance on validation set (R² improvement)
- Multi-level fusion Effect: Enhanced representation through intermediate layer feature utilization

================================================================================

4. Phase 3 Ensemble

Phase 3 improved final performance by ensembling the best performing models from Phase 1 and Phase 2.

4.1 w_demo Ensemble

Composition Models:
- Phase 1 w_demo Minimum MAE: Fold 2, convnext_small.in12k + efficientnet_b5.sw_in12k
- Phase 1 w_demo Maximum R²: Fold 4, convnext_small.in12k + convnext_small.in12k
- Phase 2 w_demo Minimum MAE: Fold 3, convnext_small.in12k + efficientnet_b5.sw_in12k
- Phase 2 w_demo Maximum R²: Fold 4, convnext_small.in12k + convnext_small.in12k

Ensemble Weights:
- [0.0, 0.8, 0.0, 0.2] (High weights on Phase 1 Maximum R² model and Phase 2 Maximum R² model)

Final Performance:

Metric    | Validation | Test
----------|------------|------
MAE       | 0.646 g/dL | 0.291 g/dL
R²        | 0.526      | -0.313
Acc@0.5   | 0.563      | 0.750
Acc@1.0   | 0.688      | 1.000
Acc@2.0   | 1.000       | 1.000

4.2 wo_demo Ensemble

Composition Models:
- Phase 1 wo_demo Minimum MAE: Fold 0, convnext_small.in12k + convnext_small.in12k
- Phase 1 wo_demo Maximum R²: Fold 0, convnext_small.in12k + convnext_small.in12k (Same as Minimum MAE)
- Phase 2 wo_demo Minimum MAE (Same as Maximum R²): Fold 0, convnext_small.in12k + convnext_small.in12k

Ensemble Weights:
- [0.0, 1.0] (High weight on Phase 2 fold 0 model)

Final Performance:

Metric    | Validation | Test
----------|------------|------
MAE       | 0.811 g/dL | 0.875 g/dL
R²        | 0.412      | -6.238
Acc@0.5   | 0.350      | 0.063
Acc@1.0   | 0.750      | 0.688
Acc@2.0   | 0.950      | 1.000

4.3 Phase 3 Summary

- w_demo Ensemble Test MAE: 0.291 g/dL (Slightly increased compared to Phase 1 best 0.208, but more stable)
- w_demo Ensemble Test Acc@1.0: 1.000 (Accuracy within ±1.0 g/dL for all samples)
- wo_demo Ensemble Test MAE: 0.875 g/dL
- wo_demo Ensemble Test Acc@1.0: 0.688
- Ensemble Effect: Achieved more robust performance by combining predictions from multiple models

================================================================================

5. Overall Performance Evolution Summary

5.1 MAE Evolution Trend

Phase              | Model                    | MAE (g/dL) | Improvement Rate
-------------------|--------------------------|------------|------------------
Single Model       | Nail (Best)              | 0.792      | Baseline
Single Model       | Conj (Best)              | 0.935      | -
Phase 1 Fusion     | w_demo (Best)            | 0.208      | 73.7% ↓
Phase 1 Fusion    | wo_demo (Best)           | 0.217      | 72.6% ↓
Phase 2 Fusion     | w_demo (Val Best)       | 0.655      | -
Phase 3 Ensemble   | w_demo (Test)           | 0.291      | 63.3% ↓ (vs Single)
Phase 3 Ensemble   | wo_demo (Test)          | 0.875      | 7.2% ↓ (vs Single)

5.2 R² Evolution Trend

Phase              | Model                    | R²    | Improvement
-------------------|--------------------------|-------|------------
Single Model       | Nail (Best)              | 0.437 | Baseline
Single Model       | Conj (Best)              | 0.356 | -
Phase 1 Fusion     | w_demo (Best)            | 0.020 | -
Phase 1 Fusion     | wo_demo (Best)           | -0.049| -
Phase 2 Fusion     | w_demo (Val Best)       | 0.478 | ↑
Phase 3 Ensemble   | w_demo (Val)            | 0.526 | ↑

5.3 Accuracy Evolution Trend

Phase              | Model                    | Acc@0.5 | Acc@1.0 | Acc@2.0
-------------------|--------------------------|---------|---------|--------
Single Model       | Nail (Best)              | 0.446   | 0.663   | 0.940
Single Model       | Conj (Best)              | 0.421   | 0.716   | 0.884
Phase 1 Fusion     | w_demo (Best)            | 1.000   | 1.000   | 1.000
Phase 3 Ensemble   | w_demo (Test)           | 0.750   | 1.000   | 1.000
Phase 3 Ensemble   | wo_demo (Test)          | 0.063   | 0.688   | 1.000

================================================================================

6. Key Findings

6.1 Single Model → Fusion

1. Significant Performance Improvement: MAE reduced by approximately 73% compared to single model
   - Nail Single: 0.792 → Phase 1 Fusion: 0.208
   - Multimodal fusion effect is very large

2. Demographics Effect: w_demo slightly outperforms wo_demo
   - w_demo: MAE 0.208
   - wo_demo: MAE 0.217

6.2 Phase 1 → Phase 2

1. R² Improvement: Validation R² significantly improved in Phase 2 (0.478)
   - Multi-level feature fusion provides richer feature representation

2. Stability Improvement: While some Phase 1 models showed negative R², Phase 2 achieved mostly positive R²

6.3 Phase 2 → Phase 3 (Ensemble)

1. Robustness Improvement: More stable performance by combining predictions from multiple models
   - w_demo Test Acc@1.0: 1.000 (within ±1.0 g/dL for all samples)
   - wo_demo Test Acc@1.0: 0.688

2. Weight Optimization: Optimal weight search through grid search
   - w_demo: High weights assigned to Phase 1 Maximum R² model and Phase 2 Maximum R² model
   - wo_demo: High weight assigned to Phase 2 model

3. Expanded Demographics Effect: The importance of Demographics information becomes even clearer at the ensemble stage
   - w_demo Test MAE: 0.291 g/dL
   - wo_demo Test MAE: 0.875 g/dL
   - Approximately 3x performance difference

================================================================================

7. Conclusion

1. Multimodal Fusion Effect: Achieved significant performance improvement with approximately 73% MAE reduction compared to single model

2. Step-by-step Improvement: Performance continuously improved through Phase 1 (Basic Fusion) → Phase 2 (Multi-level Fusion) → Phase 3 (Ensemble)

3. Final Performance:
   - w_demo Ensemble: Achieved Test MAE 0.291 g/dL and Acc@1.0 1.000, securing practical-level performance
   - wo_demo Ensemble: Test MAE 0.875 g/dL, Acc@1.0 0.688

4. Importance of Demographics Utilization: w_demo model including age and gender information showed significantly superior performance compared to wo_demo
   - w_demo Test MAE: 0.291 g/dL
   - wo_demo Test MAE: 0.875 g/dL
   - Demographics information plays a very important role in performance improvement (approximately 200% performance difference)
   - Including Demographics information improves Test MAE by approximately 3x

================================================================================

8. Future Improvement Directions

1. Data Expansion: Improve model generalization performance by collecting more patient data
2. Hyperparameter Tuning: Optimize learning rate schedule, batch size, etc.
3. Ensemble Strategy Improvement: Research on more diverse model combinations and weight search methods
4. Cross-validation Improvement: Stable performance evaluation using more folds

